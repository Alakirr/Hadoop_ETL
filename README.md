##### Hadoop_ETL
---------------------------------------------------------------------------------------------------------


---------------------------------------------------------------------------------------------------------
В рамках данного проекта реализован полноценный ETL-процесс. В качестве источника данных используется API от HeadHunter (сбор информации по актуальным вакансиям, в названии которых есть влово "Аналитик"). Сырые данные в формате CSV сохраняются в папку (можно заменить папку на S3). После чего данные очищаются и аггрегируются с использованием библиотеки Pyspark. На следующем этапе реализована загрузка полученных данных в PostgreSQL. В качестве оркестратора выступает Airflow.

---------------------------------------------------------------------------------------------------------
Проект можно запустить клонировав содержимое репозитория (git clone https://github.com/Alakirr/General_ETL.git), а затем из папки с содержимым, поднять сеть контейнеров (docker compose up -d). Дополнительно необходимо настроить дату и частоту выполнения задач (tasks) в рамках основного DAG(а) ELT, который находится в файле ETL.py. Метаданные для Airflow записываются в PostgreSQL. 

---------------------------------------------------------------------------------------------------------
Чтобы зайти в Airflow используйте логин airflow и пароль airflow. 
Для управления PostgreSQL используйте логин airflow, пароль airflow и название БД airflow. 

---------------------------------------------------------------------------------------------------------
В дальнейшем планируется расширить ETL:
1) увеличить объем данных (увеличение количества запросов к API/добавление синтетических данных);
2) добавить Superset/Grafana;
3) использовать вместо "Файловой системы" (папки на сети) S3;
4) добавить DL-модель, которая будет определять зарплатную вилку по описанию вакансии.
